![GitHub last commit](https://img.shields.io/github/last-commit/McKlay/classical-machine-learning)
![GitHub Repo stars](https://img.shields.io/github/stars/McKlay/classical-machine-learning?style=social)
![GitHub forks](https://img.shields.io/github/forks/McKlay/classical-machine-learning?style=social)
![MIT License](https://img.shields.io/github/license/McKlay/classical-machine-learning)

![Visitors](https://visitor-badge.laobi.icu/badge?page_id=McKlay.classical-machine-learning)

# Classical Machine Learning

> *"A Builder’s Guide to Mastering Traditional Algorithms with scikit-learn."*

**Live Site**: [https://mcklay.github.io/classical-machine-learning/](https://mcklay.github.io/classical-machine-learning/)  
Author: [Clay Mark Sarte](https://github.com/McKlay)  
Built with [MkDocs Material](https://squidfunk.github.io/mkdocs-material/) | Powered by scikit-learn, NumPy, and years of hands-on debugging

---

## What This Is

This repository contains a full-length, book-style guide designed to help **engineers, developers, and practitioners** master **classical machine learning algorithms** using `scikit-learn`.

It’s not just about running models. It’s about:

- Understanding the math and geometry behind algorithms like Logistic Regression, SVM, Decision Trees, and KNN
- Tracing how `scikit-learn` works under the hood, from `fit()` to pipelines and cross-validation
- Building, evaluating, and tuning models with real datasets
- Debugging performance issues and interpreting results

This handbook was born from frustration with abstraction and is written for those who think like engineers—questioning everything and wanting to go deep before going fast.

---

## Table of Contents

- `Part I – Foundations`  
  What machine learning is, anatomy of scikit-learn, pipelines, and cross-validation.

- `Part II – Core Algorithms (Supervised Learning)`  
  Dummy classifiers, Logistic/Linear Regression, KNN, Decision Trees, SVM, Naive Bayes, Random Forests, Gradient Boosting.

- `Part III – Core Algorithms (Unsupervised Learning)`  
  K-Means, Hierarchical Clustering, DBSCAN.

- `Part IV – Model Evaluation & Tuning`  
  Metrics, cross-validation, hyperparameter tuning, probability calibration, decision thresholds.

- `Part V – Data Engineering & Preprocessing`  
  Feature scaling, dimensionality reduction (PCA), handling imbalanced datasets.

- `Part VI – Advanced Topics`  
  Pipelines and workflows, under the hood of scikit-learn.

- `Appendices & Templates`  
  Glossary, scikit-learn cheat sheet, debugging tips, further reading.

---

## Built With

- Markdown (MkDocs-ready structure)  
- scikit-learn (for all algorithms and tools)  
- NumPy & Matplotlib (for math and visualizations)  
- Jupyter Notebooks (for interactive examples)

---

## Getting Started

```bash
# Clone the repo
git clone https://github.com/McKlay/classical-machine-learning.git
cd classical-machine-learning

# To view locally as a site (optional)
pip install mkdocs-material
mkdocs serve
```

You can also read the chapters directly as Markdown or view the deployed site above.

---

## Preview Chapters

Each chapter is self-contained and linked in the live site’s sidebar. You can start at the Preface or jump to the sections that fit your current needs—foundations, specific algorithms, evaluation, or preprocessing.

---

## Status

**100% Complete**  
All core chapters and examples are now published. Future updates may include:

* Interactive Jupyter notebooks
* Case studies on real-world datasets
* Advanced topics like ensemble methods

Contributions and issue reports are welcome.

---

## License

MIT License © Clay Mark Sarte  
Free to read, remix, and share, with proper attribution.

---